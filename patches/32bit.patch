diff --git a/tensorflow/contrib/cmake/external/protobuf.cmake b/tensorflow/contrib/cmake/external/protobuf.cmake
index aedb793d2a..985a5e0890 100644
--- a/tensorflow/contrib/cmake/external/protobuf.cmake
+++ b/tensorflow/contrib/cmake/external/protobuf.cmake
@@ -23,7 +23,7 @@ if(WIN32)
     debug ${CMAKE_CURRENT_BINARY_DIR}/protobuf/src/protobuf/$(Configuration)/libprotobufd.lib
     optimized ${CMAKE_CURRENT_BINARY_DIR}/protobuf/src/protobuf/$(Configuration)/libprotobuf.lib)
   set(PROTOBUF_PROTOC_EXECUTABLE ${CMAKE_CURRENT_BINARY_DIR}/protobuf/src/protobuf/$(Configuration)/protoc.exe)
-  set(PROTOBUF_ADDITIONAL_CMAKE_OPTIONS	-Dprotobuf_MSVC_STATIC_RUNTIME:BOOL=OFF -A x64)
+  set(PROTOBUF_ADDITIONAL_CMAKE_OPTIONS	-Dprotobuf_MSVC_STATIC_RUNTIME:BOOL=OFF)
 else()
   set(protobuf_STATIC_LIBRARIES ${CMAKE_CURRENT_BINARY_DIR}/protobuf/src/protobuf/libprotobuf.a)
   set(PROTOBUF_PROTOC_EXECUTABLE ${CMAKE_CURRENT_BINARY_DIR}/protobuf/src/protobuf/protoc)
diff --git a/tensorflow/contrib/cmake/tf_core_kernels.cmake b/tensorflow/contrib/cmake/tf_core_kernels.cmake
index 6927bf03f0..e05f7fc71d 100644
--- a/tensorflow/contrib/cmake/tf_core_kernels.cmake
+++ b/tensorflow/contrib/cmake/tf_core_kernels.cmake
@@ -158,21 +158,30 @@ if(WIN32)
   file(GLOB_RECURSE tf_core_kernels_windows_exclude_srcs
       # not working on windows yet
       "${tensorflow_source_dir}/tensorflow/core/kernels/neon/*"
-      # not in core - those are loaded dynamically as dll
-      "${tensorflow_source_dir}/tensorflow/contrib/nearest_neighbor/kernels/hyperplane_lsh_probes.cc"
-      "${tensorflow_source_dir}/tensorflow/contrib/nearest_neighbor/ops/nearest_neighbor_ops.cc"
-      "${tensorflow_source_dir}/tensorflow/contrib/resampler/kernels/resampler_ops.cc"
-      "${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/blas_gemm.cc"
-      "${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/gru_ops.cc"
-      "${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/lstm_ops.cc"
-      "${tensorflow_source_dir}/tensorflow/contrib/rnn/ops/gru_ops.cc"
-      "${tensorflow_source_dir}/tensorflow/contrib/rnn/ops/lstm_ops.cc"
-      "${tensorflow_source_dir}/tensorflow/contrib/seq2seq/kernels/beam_search_ops.cc"
-      "${tensorflow_source_dir}/tensorflow/contrib/seq2seq/ops/beam_search_ops.cc"
-      # temporarily disable nccl (nccl itself needs to be ported to windows first)
-      "${tensorflow_source_dir}/tensorflow/contrib/nccl/kernels/nccl_manager.cc"
-      "${tensorflow_source_dir}/tensorflow/contrib/nccl/kernels/nccl_ops.cc"
-      "${tensorflow_source_dir}/tensorflow/contrib/nccl/ops/nccl_ops.cc"
+	  "${tensorflow_source_dir}/tensorflow/core/kernels/batch_kernels.cc"
+	  "${tensorflow_source_dir}/tensorflow/core/kernels/draw_bounding_box_op.cc"
+	  "${tensorflow_source_dir}/tensorflow/core/kernels/conv_grad_ops_3d.cc"
+	  "${tensorflow_source_dir}/tensorflow/core/kernels/matrix_band_part_op.cc"
+	  "${tensorflow_source_dir}/tensorflow/core/kernels/matrix_diag_op.cc"
+	  "${tensorflow_source_dir}/tensorflow/core/kernels/resize_bicubic_op.cc"
+	  "${tensorflow_source_dir}/tensorflow/core/kernels/matrix_set_diag_op.cc"
+	  "${tensorflow_source_dir}/tensorflow/core/kernels/split_v_op.cc"
+	  "${tensorflow_source_dir}/tensorflow/core/kernels/unique_op.cc"
+    # not in core - those are loaded dynamically as dll
+    "${tensorflow_source_dir}/tensorflow/contrib/nearest_neighbor/kernels/hyperplane_lsh_probes.cc"
+    "${tensorflow_source_dir}/tensorflow/contrib/nearest_neighbor/ops/nearest_neighbor_ops.cc"
+    "${tensorflow_source_dir}/tensorflow/contrib/resampler/kernels/resampler_ops.cc"
+    "${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/blas_gemm.cc"
+    "${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/gru_ops.cc"
+    "${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/lstm_ops.cc"
+    "${tensorflow_source_dir}/tensorflow/contrib/rnn/ops/gru_ops.cc"
+    "${tensorflow_source_dir}/tensorflow/contrib/rnn/ops/lstm_ops.cc"
+    "${tensorflow_source_dir}/tensorflow/contrib/seq2seq/kernels/beam_search_ops.cc"
+    "${tensorflow_source_dir}/tensorflow/contrib/seq2seq/ops/beam_search_ops.cc"
+    # temporarily disable nccl (nccl itself needs to be ported to windows first)
+    "${tensorflow_source_dir}/tensorflow/contrib/nccl/kernels/nccl_manager.cc"
+    "${tensorflow_source_dir}/tensorflow/contrib/nccl/kernels/nccl_ops.cc"
+    "${tensorflow_source_dir}/tensorflow/contrib/nccl/ops/nccl_ops.cc"
   )
   list(REMOVE_ITEM tf_core_kernels_srcs ${tf_core_kernels_windows_exclude_srcs})
 endif(WIN32)
diff --git a/tensorflow/contrib/cmake/tools/create_def_file.py b/tensorflow/contrib/cmake/tools/create_def_file.py
index 77ea914380..8ff94233ef 100644
--- a/tensorflow/contrib/cmake/tools/create_def_file.py
+++ b/tensorflow/contrib/cmake/tools/create_def_file.py
@@ -126,7 +126,7 @@ def main():
     # Header for the def file.
     def_fp.write("LIBRARY " + args.target + "\n")
     def_fp.write("EXPORTS\n")
-    def_fp.write("\t ??1OpDef@tensorflow@@UEAA@XZ\n")
+    def_fp.write("\t ??1OpDef@tensorflow@@UAE@XZ\n")
 
     # Each symbols returned by undname matches the same position in candidates.
     # We compare on undname but use the decorated name from candidates.
diff --git a/tensorflow/core/common_runtime/bfc_allocator.h b/tensorflow/core/common_runtime/bfc_allocator.h
index 9353997753..ec13c29e9b 100644
--- a/tensorflow/core/common_runtime/bfc_allocator.h
+++ b/tensorflow/core/common_runtime/bfc_allocator.h
@@ -380,7 +380,7 @@ class BFCAllocator : public VisitableAllocator {
     return 63 ^ __builtin_clzll(n);
 #elif defined(PLATFORM_WINDOWS)
     unsigned long index;
-    _BitScanReverse64(&index, n);
+    _BitScanReverse(&index, n);
     return index;
 #else
     return Log2FloorNonZeroSlow(n);
diff --git a/tensorflow/core/kernels/resize_bilinear_op.cc b/tensorflow/core/kernels/resize_bilinear_op.cc
index d9cb993a4b..122b66604d 100644
--- a/tensorflow/core/kernels/resize_bilinear_op.cc
+++ b/tensorflow/core/kernels/resize_bilinear_op.cc
@@ -67,22 +67,22 @@ class ResizeBilinearOp : public OpKernel {
 namespace {
 // Compute the interpolation indices only once.
 struct CachedInterpolation {
-  int64 lower;  // Lower source index used in the interpolation
-  int64 upper;  // Upper source index used in the interpolation
+  int32 lower;  // Lower source index used in the interpolation
+  int32 upper;  // Upper source index used in the interpolation
   // 1-D linear iterpolation scale (see:
   // https://en.wikipedia.org/wiki/Bilinear_interpolation)
   float lerp;
 };
 
-inline void compute_interpolation_weights(const int64 out_size,
-                                          const int64 in_size,
+inline void compute_interpolation_weights(const int32 out_size,
+                                          const int32 in_size,
                                           const float scale,
                                           CachedInterpolation* interpolation) {
   interpolation[out_size].lower = 0;
   interpolation[out_size].upper = 0;
-  for (int64 i = out_size - 1; i >= 0; --i) {
+  for (int32 i = out_size - 1; i >= 0; --i) {
     const float in = i * scale;
-    interpolation[i].lower = static_cast<int64>(in);
+    interpolation[i].lower = static_cast<int32>(in);
     interpolation[i].upper = std::min(interpolation[i].lower + 1, in_size - 1);
     interpolation[i].lerp = in - interpolation[i].lower;
   }
@@ -103,22 +103,22 @@ inline float compute_lerp(const float top_left, const float top_right,
 template <typename T>
 void resize_image(
     typename TTypes<T, 4>::ConstTensor images, const int batch_size,
-    const int64 in_height, const int64 in_width, const int64 out_height,
-    const int64 out_width, const int channels,
+    const int32 in_height, const int32 in_width, const int32 out_height,
+    const int32 out_width, const int channels,
     const std::vector<CachedInterpolation>& xs,
     const std::vector<CachedInterpolation>& ys,
     typename TTypes<float, 4>::Tensor output) TF_ATTRIBUTE_NOINLINE;
 template <typename T>
 void resize_image(typename TTypes<T, 4>::ConstTensor images,
-                  const int batch_size, const int64 in_height,
-                  const int64 in_width, const int64 out_height,
-                  const int64 out_width, const int channels,
+                  const int batch_size, const int32 in_height,
+                  const int32 in_width, const int32 out_height,
+                  const int32 out_width, const int channels,
                   const std::vector<CachedInterpolation>& xs_vec,
                   const std::vector<CachedInterpolation>& ys,
                   typename TTypes<float, 4>::Tensor output) {
-  const int64 in_row_size = in_width * channels;
-  const int64 in_batch_num_values = in_height * in_row_size;
-  const int64 out_row_size = out_width * channels;
+  const int32 in_row_size = in_width * channels;
+  const int32 in_batch_num_values = in_height * in_row_size;
+  const int32 out_row_size = out_width * channels;
 
   const T* input_b_ptr = images.data();
   const CachedInterpolation* xs = xs_vec.data();
@@ -126,13 +126,13 @@ void resize_image(typename TTypes<T, 4>::ConstTensor images,
   if (channels == 3) {
     float* output_y_ptr = output.data();
     for (int b = 0; b < batch_size; ++b) {
-      for (int64 y = 0; y < out_height; ++y) {
+      for (int32 y = 0; y < out_height; ++y) {
         const T* ys_input_lower_ptr = input_b_ptr + ys[y].lower * in_row_size;
         const T* ys_input_upper_ptr = input_b_ptr + ys[y].upper * in_row_size;
         const float ys_lerp = ys[y].lerp;
-        for (int64 x = 0; x < out_width; ++x) {
-          const int64 xs_lower = xs[x].lower;
-          const int64 xs_upper = xs[x].upper;
+        for (int32 x = 0; x < out_width; ++x) {
+          const int32 xs_lower = xs[x].lower;
+          const int32 xs_upper = xs[x].upper;
           const float xs_lerp = xs[x].lerp;
 
           // Read channel 0.
@@ -171,11 +171,11 @@ void resize_image(typename TTypes<T, 4>::ConstTensor images,
   } else {
     float* output_y_ptr = output.data();
     for (int b = 0; b < batch_size; ++b) {
-      for (int64 y = 0; y < out_height; ++y) {
+      for (int32 y = 0; y < out_height; ++y) {
         const T* ys_input_lower_ptr = input_b_ptr + ys[y].lower * in_row_size;
         const T* ys_input_upper_ptr = input_b_ptr + ys[y].upper * in_row_size;
         const float ys_lerp = ys[y].lerp;
-        for (int64 x = 0; x < out_width; ++x) {
+        for (int32 x = 0; x < out_width; ++x) {
           auto xs_lower = xs[x].lower;
           auto xs_upper = xs[x].upper;
           auto xs_lerp = xs[x].lerp;
@@ -206,12 +206,12 @@ struct ResizeBilinear<CPUDevice, T> {
                   const float height_scale, const float width_scale,
                   typename TTypes<float, 4>::Tensor output) {
     const int batch_size = images.dimension(0);
-    const int64 in_height = images.dimension(1);
-    const int64 in_width = images.dimension(2);
+    const int32 in_height = images.dimension(1);
+    const int32 in_width = images.dimension(2);
     const int channels = images.dimension(3);
 
-    const int64 out_height = output.dimension(1);
-    const int64 out_width = output.dimension(2);
+    const int32 out_height = output.dimension(1);
+    const int32 out_width = output.dimension(2);
 
     // Handle no-op resizes efficiently.
     if (out_height == in_height && out_width == in_width) {
@@ -280,12 +280,12 @@ struct ResizeBilinearGrad<CPUDevice, T> {
                   const float height_scale, const float width_scale,
                   typename TTypes<T, 4>::Tensor output_grad) {
     const int batch = output_grad.dimension(0);
-    const int64 original_height = output_grad.dimension(1);
-    const int64 original_width = output_grad.dimension(2);
+    const int32 original_height = output_grad.dimension(1);
+    const int32 original_width = output_grad.dimension(2);
     const int channels = output_grad.dimension(3);
 
-    const int64 resized_height = input_grad.dimension(1);
-    const int64 resized_width = input_grad.dimension(2);
+    const int32 resized_height = input_grad.dimension(1);
+    const int32 resized_width = input_grad.dimension(2);
 
     output_grad.setZero();
 
@@ -296,22 +296,22 @@ struct ResizeBilinearGrad<CPUDevice, T> {
     //                       +  top_right * (1 - y) * x
     //                       +  bottom_left * y * (1 - x)
     //                       +  bottom_right * y * x
-    for (int64 b = 0; b < batch; ++b) {
-      for (int64 y = 0; y < resized_height; ++y) {
+    for (int32 b = 0; b < batch; ++b) {
+      for (int32 y = 0; y < resized_height; ++y) {
         const float in_y = y * height_scale;
-        const int64 top_y_index = static_cast<int64>(floorf(in_y));
-        const int64 bottom_y_index =
-            std::min(static_cast<int64>(ceilf(in_y)), original_height - 1);
+        const int32 top_y_index = static_cast<int32>(floorf(in_y));
+        const int32 bottom_y_index =
+            std::min(static_cast<int32>(ceilf(in_y)), original_height - 1);
         const float y_lerp = in_y - top_y_index;
         const float inverse_y_lerp = (1.0f - y_lerp);
-        for (int64 x = 0; x < resized_width; ++x) {
+        for (int32 x = 0; x < resized_width; ++x) {
           const float in_x = x * width_scale;
-          const int64 left_x_index = static_cast<int64>(floorf(in_x));
-          const int64 right_x_index =
-              std::min(static_cast<int64>(ceilf(in_x)), original_width - 1);
+          const int32 left_x_index = static_cast<int32>(floorf(in_x));
+          const int32 right_x_index =
+              std::min(static_cast<int32>(ceilf(in_x)), original_width - 1);
           const float x_lerp = in_x - left_x_index;
           const float inverse_x_lerp = (1.0f - x_lerp);
-          for (int64 c = 0; c < channels; ++c) {
+          for (int32 c = 0; c < channels; ++c) {
             output_grad(b, top_y_index, left_x_index, c) +=
                 T(input_grad(b, y, x, c) * inverse_y_lerp * inverse_x_lerp);
             output_grad(b, top_y_index, right_x_index, c) +=
diff --git a/tensorflow/core/kernels/resize_nearest_neighbor_op.cc b/tensorflow/core/kernels/resize_nearest_neighbor_op.cc
index bfd29b7ec8..ed25bbe6f9 100644
--- a/tensorflow/core/kernels/resize_nearest_neighbor_op.cc
+++ b/tensorflow/core/kernels/resize_nearest_neighbor_op.cc
@@ -89,23 +89,23 @@ struct ResizeNearestNeighbor<CPUDevice, T, align_corners> {
                   const float height_scale, const float width_scale,
                   typename TTypes<T, 4>::Tensor output) {
     const int batch_size = input.dimension(0);
-    const int64 in_height = input.dimension(1);
-    const int64 in_width = input.dimension(2);
+    const int32 in_height = input.dimension(1);
+    const int32 in_width = input.dimension(2);
     const int channels = input.dimension(3);
 
-    const int64 out_height = output.dimension(1);
-    const int64 out_width = output.dimension(2);
+    const int32 out_height = output.dimension(1);
+    const int32 out_width = output.dimension(2);
 
     for (int b = 0; b < batch_size; ++b) {
       for (int y = 0; y < out_height; ++y) {
-        const int64 in_y = std::min(
-            (align_corners) ? static_cast<int64>(roundf(y * height_scale))
-                            : static_cast<int64>(floorf(y * height_scale)),
+        const int32 in_y = std::min(
+            (align_corners) ? static_cast<int32>(roundf(y * height_scale))
+                            : static_cast<int32>(floorf(y * height_scale)),
             in_height - 1);
         for (int x = 0; x < out_width; ++x) {
-          const int64 in_x = std::min(
-              (align_corners) ? static_cast<int64>(roundf(x * width_scale))
-                              : static_cast<int64>(floorf(x * width_scale)),
+          const int32 in_x = std::min(
+              (align_corners) ? static_cast<int32>(roundf(x * width_scale))
+                              : static_cast<int32>(floorf(x * width_scale)),
               in_width - 1);
           std::copy_n(&input(b, in_y, in_x, 0), channels, &output(b, y, x, 0));
         }
@@ -144,13 +144,13 @@ class ResizeNearestNeighborOpGrad : public OpKernel {
     OP_REQUIRES(context, sizes(0) > 0 && sizes(1) > 0,
                 errors::InvalidArgument("shape_t's elements must be positive"));
 
-    const int64 batch_size = input.dim_size(0);
-    const int64 in_height = input.dim_size(1);
-    const int64 in_width = input.dim_size(2);
-    const int64 channels = input.dim_size(3);
+    const int32 batch_size = input.dim_size(0);
+    const int32 in_height = input.dim_size(1);
+    const int32 in_width = input.dim_size(2);
+    const int32 channels = input.dim_size(3);
 
-    const int64 out_height = sizes(0);
-    const int64 out_width = sizes(1);
+    const int32 out_height = sizes(0);
+    const int32 out_width = sizes(1);
 
     Tensor* output = nullptr;
     OP_REQUIRES_OK(
@@ -200,24 +200,24 @@ struct ResizeNearestNeighborGrad<CPUDevice, T, align_corners> {
                   const float height_scale, const float width_scale,
                   typename TTypes<T, 4>::Tensor output) {
     const int batch_size = input.dimension(0);
-    const int64 in_height = input.dimension(1);
-    const int64 in_width = input.dimension(2);
+    const int32 in_height = input.dimension(1);
+    const int32 in_width = input.dimension(2);
     const int channels = input.dimension(3);
 
-    const int64 out_height = output.dimension(1);
-    const int64 out_width = output.dimension(2);
+    const int32 out_height = output.dimension(1);
+    const int32 out_width = output.dimension(2);
 
     output.setZero();
 
     for (int y = 0; y < in_height; ++y) {
-      const int64 out_y = std::min(
-          (align_corners) ? static_cast<int64>(roundf(y * height_scale))
-                          : static_cast<int64>(floorf(y * height_scale)),
+      const int32 out_y = std::min(
+          (align_corners) ? static_cast<int32>(roundf(y * height_scale))
+                          : static_cast<int32>(floorf(y * height_scale)),
           out_height - 1);
       for (int x = 0; x < in_width; ++x) {
-        const int64 out_x = std::min(
-            (align_corners) ? static_cast<int64>(roundf(x * width_scale))
-                            : static_cast<int64>(floorf(x * width_scale)),
+        const int32 out_x = std::min(
+            (align_corners) ? static_cast<int32>(roundf(x * width_scale))
+                            : static_cast<int32>(floorf(x * width_scale)),
             out_width - 1);
         for (int b = 0; b < batch_size; ++b) {
           for (int c = 0; c < channels; ++c) {
diff --git a/tensorflow/core/kernels/split_op.cc b/tensorflow/core/kernels/split_op.cc
index 85f529326d..1ff1727339 100644
--- a/tensorflow/core/kernels/split_op.cc
+++ b/tensorflow/core/kernels/split_op.cc
@@ -89,7 +89,7 @@ class SplitOpBase : public OpKernel {
     // the copying.
     if ((split_dim == 0) && IsInnerDimsSizeAligned<T>(input_shape)) {
       VLOG(1) << "Slice dim 0: " << input_shape.DebugString();
-      const int64 delta = input_shape.dim_size(0) / num_split;
+      const int32 delta = input_shape.dim_size(0) / num_split;
       for (int i = 0; i < num_split; ++i) {
         context->set_output(i, input.Slice(i * delta, (i + 1) * delta));
       }
@@ -157,7 +157,7 @@ class SplitOpCPU : public SplitOpBase<CPUDevice, T> {
     auto input_reshaped =
         input.shaped<T, 3>({prefix_dim_size, split_dim_size, suffix_dim_size});
 
-    const int64 split_dim_output_size = split_dim_size / num_split;
+    const int32 split_dim_output_size = split_dim_size / num_split;
     TensorShape output_shape(input_shape);
     output_shape.set_dim(split_dim, split_dim_output_size);
 
@@ -177,8 +177,8 @@ class SplitOpCPU : public SplitOpBase<CPUDevice, T> {
     auto range_output_func = [&indices, context, &output_shape, prefix_dim_size,
                               split_dim_output_size, suffix_dim_size, &sizes,
                               use_parallelism_between_outputs,
-                              &input_reshaped](int64 start, int64 limit) {
-      for (int64 i = start; i < limit; ++i) {
+                              &input_reshaped](int32 start, int32 limit) {
+      for (int32 i = start; i < limit; ++i) {
         Tensor* result = nullptr;
         OP_REQUIRES_OK(context,
                        context->allocate_output(i, output_shape, &result));
@@ -321,7 +321,7 @@ class SplitOpSYCL : public SplitOpBase<SYCLDevice, T> {
     auto input_reshaped =
         input.shaped<T, 3>({prefix_dim_size, split_dim_size, suffix_dim_size});
 
-    const int64 split_dim_output_size = split_dim_size / num_split;
+    const int32 split_dim_output_size = split_dim_size / num_split;
     TensorShape output_shape(input_shape);
     output_shape.set_dim(split_dim, split_dim_output_size);
 
diff --git a/tensorflow/core/kernels/tensor_array_ops.cc b/tensorflow/core/kernels/tensor_array_ops.cc
index af93d814ec..4cb10e5f1b 100644
--- a/tensorflow/core/kernels/tensor_array_ops.cc
+++ b/tensorflow/core/kernels/tensor_array_ops.cc
@@ -840,9 +840,9 @@ class TensorArrayConcatOp : public OpKernel {
 
     Tensor* lengths_tensor = nullptr;
     OP_REQUIRES_OK(ctx, ctx->allocate_output(
-                            1, TensorShape({static_cast<int64>(values.size())}),
+                            1, TensorShape({static_cast<int32>(values.size())}),
                             &lengths_tensor));
-    auto lengths_tensor_t = lengths_tensor->vec<int64>();
+    auto lengths_tensor_t = lengths_tensor->vec<int32>();
 
     TensorShape output_shape;
     TensorShape output_shape_except0;
@@ -1088,7 +1088,7 @@ class TensorArrayUnpackOrScatterOp : public OpKernel {
 
     Eigen::DSizes<Eigen::DenseIndex, 3> indices{0, 0, 0};
     Eigen::DSizes<Eigen::DenseIndex, 3> sizes{1, 1,
-                                              element_shape.num_elements()};
+                                              static_cast<int32>(element_shape.num_elements())};
 
     std::vector<PersistentTensor> write_values;
     write_values.reserve(num_values);
@@ -1220,10 +1220,10 @@ class TensorArraySplitOp : public OpKernel {
                     "Expected lengths to have < max int32 entries"));
 
     int32 num_tensors = static_cast<int32>(tensor_lengths->NumElements());
-    auto tensor_lengths_t = tensor_lengths->vec<int64>();
-    std::vector<int64> cumulative_lengths;
+    auto tensor_lengths_t = tensor_lengths->vec<int32>();
+    std::vector<int32> cumulative_lengths;
     cumulative_lengths.reserve(num_tensors);
-    int64 total_length = 0;
+    int32 total_length = 0;
     for (int i = 0; i < num_tensors; ++i) {
       total_length += tensor_lengths_t(i);
       cumulative_lengths.push_back(total_length);
@@ -1241,7 +1241,7 @@ class TensorArraySplitOp : public OpKernel {
                                 "values.shape[0], but sum of lengths is ",
                                 total_length, " and value's shape is: ",
                                 tensor_value->shape().DebugString()));
-    int64 elements_per_row =
+    int32 elements_per_row =
         (total_length == 0) ? 0 : (tensor_value->NumElements() / total_length);
 
     int32 array_size;
@@ -1282,7 +1282,7 @@ class TensorArraySplitOp : public OpKernel {
       Tensor* tensor_value_i;
       PersistentTensor persistent_tensor;
 
-      int64 previous_length = (i == 0) ? 0 : cumulative_lengths[i - 1];
+      int previous_length = (i == 0) ? 0 : cumulative_lengths[i - 1];
       Eigen::DSizes<Eigen::DenseIndex, 3> indices{0, previous_length, 0};
       Eigen::DSizes<Eigen::DenseIndex, 3> sizes{1, tensor_lengths_t(i),
                                                 elements_per_row};
diff --git a/tensorflow/core/kernels/unpack_op.cc b/tensorflow/core/kernels/unpack_op.cc
index 764b6a252a..4989239705 100644
--- a/tensorflow/core/kernels/unpack_op.cc
+++ b/tensorflow/core/kernels/unpack_op.cc
@@ -63,7 +63,7 @@ class UnpackOp : public OpKernel {
 
     auto output_shape = input_shape;
     output_shape.RemoveDim(axis);
-    const int64 output_size = output_shape.num_elements();
+    const int32 output_size = output_shape.num_elements();
     OP_REQUIRES(
         context,
         FastBoundsCheck(output_size,
@@ -90,16 +90,16 @@ class UnpackOp : public OpKernel {
     }
 #endif  // TENSORFLOW_USE_SYCL
 
-    int64 before_dim = 1;
+    int32 before_dim = 1;
     for (int i = 0; i < axis; ++i) {
       before_dim *= input_shape.dim_size(i);
     }
 
-    int64 after_dim = 1;
+    int32 after_dim = 1;
     for (int i = axis + 1; i < input_shape.dims(); ++i) {
       after_dim *= input_shape.dim_size(i);
     }
-    const int64 axis_dim = input_shape.dim_size(axis);
+    const int32 axis_dim = input_shape.dim_size(axis);
 
     // Except for shape, unpack is a special case of split, so we reuse the
     // same computational kernels.
@@ -159,8 +159,8 @@ REGISTER_KERNEL_BUILDER(Name("Unpack")
                             .Device(DEVICE_GPU)
                             .HostMemory("value")
                             .HostMemory("output")
-                            .TypeConstraint<int64>("T"),
-                        UnpackOp<CPUDevice, int64>);
+                            .TypeConstraint<int32>("T"),
+                        UnpackOp<CPUDevice, int32>);
 
 #endif  // GOOGLE_CUDA
 
@@ -183,8 +183,8 @@ REGISTER_KERNEL_BUILDER(Name("Unpack")
                             .Device(DEVICE_SYCL)
                             .HostMemory("value")
                             .HostMemory("output")
-                            .TypeConstraint<int64>("T"),
-                        UnpackOp<CPUDevice, int64>);
+                            .TypeConstraint<int32>("T"),
+                        UnpackOp<CPUDevice, int32>);
 #undef REGISTER_SYCL
 #endif  // TENSORFLOW_USE_SYCL
 
